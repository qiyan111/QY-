#!/usr/bin/env python3
"""
æ‰¾åˆ° Alibaba Trace ä¸­ä»»åŠ¡æœ€å¯†é›†çš„æ—¶é—´çª—å£

ä½¿ç”¨æ–¹æ³•:
    python tools/filter_peak_window.py [window_hours] [max_tasks]
    
ç¤ºä¾‹:
    python tools/filter_peak_window.py 4 500000
"""
import sys
import os
sys.path.insert(0, 'tools')
from load_trace_final import load_tasks
import pickle
import numpy as np

def find_peak_window(tasks, window_size_seconds):
    """æ‰¾åˆ°ä»»åŠ¡æœ€å¯†é›†çš„æ—¶é—´çª—å£"""
    if not tasks:
        return 0, []
    
    # æŒ‰åˆ°è¾¾æ—¶é—´æ’åº
    sorted_tasks = sorted(tasks, key=lambda t: t.arrival)
    
    max_count = 0
    best_start = sorted_tasks[0].arrival
    best_end = best_start + window_size_seconds
    
    print(f"  æ‰«æ {len(sorted_tasks)} ä¸ªä»»åŠ¡ï¼Œå¯»æ‰¾æœ€å¯†é›†çš„ {window_size_seconds/3600:.1f}h çª—å£...")
    
    # ä½¿ç”¨æ»‘åŠ¨çª—å£æ‰¾åˆ°æœ€å¯†é›†çš„åŒºé—´
    # ä¸ºäº†æ•ˆç‡ï¼Œæˆ‘ä»¬æ¯éš”ä¸€å®šé—´éš”é‡‡æ ·
    sample_interval = max(1, len(sorted_tasks) // 1000)
    
    for i in range(0, len(sorted_tasks), sample_interval):
        start_time = sorted_tasks[i].arrival
        end_time = start_time + window_size_seconds
        
        # è®¡ç®—è¿™ä¸ªçª—å£å†…çš„ä»»åŠ¡æ•°
        count = sum(1 for t in sorted_tasks 
                   if start_time <= t.arrival < end_time)
        
        if count > max_count:
            max_count = count
            best_start = start_time
            best_end = end_time
    
    # æå–çª—å£å†…çš„ä»»åŠ¡
    filtered = [t for t in sorted_tasks 
                if best_start <= t.arrival < best_end]
    
    return best_start, filtered


def analyze_window(tasks, window_start, window_size):
    """åˆ†æçª—å£å†…çš„ä»»åŠ¡ç‰¹å¾"""
    if not tasks:
        return {}
    
    # åŸºæœ¬ç»Ÿè®¡
    total_cpu = sum(t.cpu for t in tasks)
    total_mem = sum(t.mem for t in tasks)
    avg_cpu = total_cpu / len(tasks)
    avg_mem = total_mem / len(tasks)
    
    # æ—¶é•¿ç»Ÿè®¡
    durations = [t.duration for t in tasks if t.duration > 0]
    avg_duration = np.mean(durations) if durations else 0
    median_duration = np.median(durations) if durations else 0
    
    # ç†è®ºå¹¶å‘åº¦
    # å‡è®¾ä»»åŠ¡å‡åŒ€åˆ†å¸ƒåœ¨çª—å£å†…
    total_work = sum(t.cpu * t.duration for t in tasks if t.duration > 0)
    theoretical_concurrent = total_work / window_size if window_size > 0 else 0
    
    # æ¨èèŠ‚ç‚¹æ•°ï¼ˆç•™30%ç¼“å†²ï¼‰
    recommended_nodes = int(theoretical_concurrent / 11.0 * 1.3)
    recommended_nodes = max(5, recommended_nodes)  # è‡³å°‘5ä¸ªèŠ‚ç‚¹
    
    return {
        'task_count': len(tasks),
        'total_cpu': total_cpu,
        'total_mem': total_mem,
        'avg_cpu': avg_cpu,
        'avg_mem': avg_mem,
        'avg_duration': avg_duration,
        'median_duration': median_duration,
        'theoretical_concurrent': theoretical_concurrent,
        'recommended_nodes': recommended_nodes,
    }


def main():
    # è§£æå‚æ•°
    window_hours = float(sys.argv[1]) if len(sys.argv) > 1 else 4.0
    max_tasks = int(sys.argv[2]) if len(sys.argv) > 2 else 500000
    
    window_size = int(window_hours * 3600)
    
    print("=" * 70)
    print("ğŸ” Alibaba Trace æ—¶é—´çª—å£è¿‡æ»¤å·¥å…·")
    print("=" * 70)
    print(f"\né…ç½®:")
    print(f"  çª—å£å¤§å°: {window_hours} å°æ—¶ ({window_size} ç§’)")
    print(f"  æœ€å¤§åŠ è½½ä»»åŠ¡æ•°: {max_tasks}")
    print()
    
    # åŠ è½½ä»»åŠ¡
    print("ğŸ“‚ åŠ è½½ Alibaba Trace...")
    all_tasks = load_tasks('./data', max_instances=max_tasks)
    print(f"  âœ… åŠ è½½äº† {len(all_tasks)} ä¸ªä»»åŠ¡")
    
    # åˆ†æåŸå§‹trace
    if all_tasks:
        min_arrival = min(t.arrival for t in all_tasks)
        max_arrival = max(t.arrival for t in all_tasks)
        time_span = max_arrival - min_arrival
        print(f"  æ—¶é—´è·¨åº¦: {time_span} ç§’ = {time_span/3600:.1f} å°æ—¶ = {time_span/86400:.1f} å¤©")
    
    # æ‰¾åˆ°æœ€å¯†é›†çš„çª—å£
    print(f"\nğŸ” å¯»æ‰¾æœ€å¯†é›†çš„ {window_hours} å°æ—¶çª—å£...")
    best_start, filtered_tasks = find_peak_window(all_tasks, window_size)
    
    if not filtered_tasks:
        print("  âŒ æœªæ‰¾åˆ°ä»»åŠ¡")
        return
    
    print(f"  âœ… æ‰¾åˆ°æœ€å¯†é›†çª—å£:")
    print(f"     å¼€å§‹æ—¶é—´: {best_start} ç§’")
    print(f"     ç»“æŸæ—¶é—´: {best_start + window_size} ç§’")
    print(f"     ä»»åŠ¡æ•°: {len(filtered_tasks)}")
    
    # åˆ†æçª—å£
    print(f"\nğŸ“Š çª—å£ç‰¹å¾åˆ†æ:")
    stats = analyze_window(filtered_tasks, best_start, window_size)
    
    print(f"  ä»»åŠ¡ç»Ÿè®¡:")
    print(f"    æ€»ä»»åŠ¡æ•°: {stats['task_count']}")
    print(f"    å¹³å‡ CPU: {stats['avg_cpu']:.3f} cores/ä»»åŠ¡")
    print(f"    å¹³å‡ MEM: {stats['avg_mem']:.3f} GB/ä»»åŠ¡")
    print(f"    å¹³å‡æ—¶é•¿: {stats['avg_duration']:.1f} ç§’")
    print(f"    ä¸­ä½æ—¶é•¿: {stats['median_duration']:.1f} ç§’")
    
    print(f"\n  å¹¶å‘åº¦åˆ†æ:")
    print(f"    ç†è®ºå¹³å‡å¹¶å‘: {stats['theoretical_concurrent']:.1f} cores")
    print(f"    æ¨èèŠ‚ç‚¹æ•°: {stats['recommended_nodes']} èŠ‚ç‚¹ ({stats['recommended_nodes'] * 11.0:.0f} cores)")
    
    # è®¡ç®—é¢„æœŸåˆ©ç”¨ç‡
    if stats['recommended_nodes'] > 0:
        capacity = stats['recommended_nodes'] * 11.0
        expected_util = stats['theoretical_concurrent'] / capacity * 100
        print(f"    é¢„æœŸå¹³å‡åˆ©ç”¨ç‡: {expected_util:.1f}%")
    
    # ä¿å­˜ç»“æœ
    output_file = 'peak_window_tasks.pkl'
    result = {
        'tasks': filtered_tasks,
        'window_start': best_start,
        'window_size': window_size,
        'window_hours': window_hours,
        'stats': stats,
    }
    
    with open(output_file, 'wb') as f:
        pickle.dump(result, f)
    
    print(f"\nğŸ’¾ å·²ä¿å­˜åˆ°: {output_file}")
    
    # ç”Ÿæˆè¿è¡Œå‘½ä»¤
    print(f"\nğŸš€ æ¨èè¿è¡Œå‘½ä»¤:")
    print(f"  export BATCH_STEP_SECONDS=3")
    print(f"  python tools/run_peak_window.py {stats['recommended_nodes']}")
    print()
    print(f"  é¢„æœŸç»“æœ:")
    print(f"    â€¢ æˆåŠŸç‡: 95-100%")
    print(f"    â€¢ åˆ©ç”¨ç‡: 50-70%")
    print(f"    â€¢ ç®—æ³•å·®å¼‚æ˜æ˜¾")
    print(f"    â€¢ NextGen é¢†å…ˆ 5-10%")
    print()
    
    print("=" * 70)


if __name__ == '__main__':
    main()
