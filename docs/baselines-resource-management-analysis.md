# ğŸ” Baseline ç®—æ³•èµ„æºç®¡ç†ä¸é‡è¯•æœºåˆ¶åˆ†æ

## ğŸ“‹ æ‘˜è¦

é€šè¿‡æ·±å…¥åˆ†æ **Firmament (OSDI'16)** å’Œ **Mesos DRF (NSDI'11)** çš„æºç ï¼Œæˆ‘ä»¬å‘ç°ï¼š

1. âœ… **ä¸¤è€…éƒ½æœ‰å®Œå–„çš„èµ„æºé‡Šæ”¾æœºåˆ¶**
2. âœ… **ä¸¤è€…éƒ½æ”¯æŒä»»åŠ¡å¤±è´¥åçš„é‡è¯•**
3. âŒ **æˆ‘ä»¬çš„æ¨¡æ‹Ÿå®ç°ä¸çœŸå®ç³»ç»Ÿå­˜åœ¨é‡å¤§å·®å¼‚**

---

## 1ï¸âƒ£ Firmament èµ„æºç®¡ç†æœºåˆ¶

### æºç ä½ç½®
- `baselines/firmament/src/scheduling/flow/flow_scheduler.cc`
- `baselines/firmament/src/scheduling/event_driven_scheduler.cc`

### æ ¸å¿ƒå‡½æ•°

#### 1.1 ä»»åŠ¡å®Œæˆ - èµ„æºé‡Šæ”¾
```cpp
void FlowScheduler::HandleTaskCompletion(TaskDescriptor* td_ptr,
                                         TaskFinalReport* report) {
  // 1. è°ƒç”¨çˆ¶ç±»å¤„ç†ï¼ˆEventDrivenSchedulerï¼‰
  EventDrivenScheduler::HandleTaskCompletion(td_ptr, report);
  
  // 2. ä» Flow Graph ä¸­ç§»é™¤ä»»åŠ¡èŠ‚ç‚¹
  uint64_t task_node_id = flow_graph_manager_->TaskCompleted(td_ptr->uid());
  tasks_completed_during_solver_run_.insert(task_node_id);
}
```

çˆ¶ç±»å®ç°ï¼š
```cpp
void EventDrivenScheduler::HandleTaskCompletion(TaskDescriptor* td_ptr,
                                                TaskFinalReport* report) {
  // 1. æ‰¾åˆ°ä»»åŠ¡ç»‘å®šçš„èµ„æº
  ResourceID_t* res_id_ptr = BoundResourceForTask(td_ptr->uid());
  ResourceStatus* rs_ptr = FindPtrOrNull(*resource_map_, res_id_tmp);
  
  // 2. â­ è§£ç»‘ä»»åŠ¡å¹¶é‡Šæ”¾èµ„æº
  CHECK(UnbindTaskFromResource(td_ptr, res_id_tmp));
  
  // 3. é€šçŸ¥ executor å¤„ç†ä»»åŠ¡å®Œæˆ
  exec->HandleTaskCompletion(td_ptr, report);
}
```

#### 1.2 èµ„æºè§£ç»‘ - å…³é”®å®ç°
```cpp
bool EventDrivenScheduler::UnbindTaskFromResource(TaskDescriptor* td_ptr,
                                                   ResourceID_t res_id) {
  TaskID_t task_id = td_ptr->uid();
  
  // 1. è·å–èµ„æºçŠ¶æ€
  ResourceStatus* rs_ptr = FindPtrOrNull(*resource_map_, res_id);
  ResourceDescriptor* rd_ptr = rs_ptr->mutable_descriptor();
  
  // 2. â­ è®¾ç½®èµ„æºä¸ºç©ºé—²çŠ¶æ€
  if (rd_ptr->current_running_tasks_size() == 0) {
    rd_ptr->set_state(ResourceDescriptor::RESOURCE_IDLE);
  }
  
  // 3. ä»ä»»åŠ¡ç»‘å®šè¡¨ä¸­ç§»é™¤
  task_bindings_.erase(task_id);
  
  return true;
}
```

### 1.3 ä»»åŠ¡é©±é€ï¼ˆEvictionï¼‰- æ”¯æŒé‡è°ƒåº¦
```cpp
void EventDrivenScheduler::HandleTaskEviction(TaskDescriptor* td_ptr,
                                              ResourceDescriptor* rd_ptr) {
  ResourceID_t res_id = ResourceIDFromString(rd_ptr->uuid());
  
  // 1. â­ è§£ç»‘å¹¶é‡Šæ”¾èµ„æº
  CHECK(UnbindTaskFromResource(td_ptr, res_id));
  
  // 2. â­ ä»»åŠ¡æ ‡è®°ä¸º RUNNABLEï¼Œé‡æ–°åŠ å…¥è°ƒåº¦é˜Ÿåˆ—
  td_ptr->set_state(TaskDescriptor::RUNNABLE);
  InsertTaskIntoRunnables(JobIDFromString(td_ptr->job_id()), td_ptr->uid());
  
  // 3. é€šçŸ¥ executor å¤„ç†é©±é€
  exec->HandleTaskEviction(td_ptr);
}
```

### 1.4 ä»»åŠ¡å¤±è´¥ - è‡ªåŠ¨é‡è¯•
```cpp
void EventDrivenScheduler::HandleTaskFailure(TaskDescriptor* td_ptr) {
  // 1. è§£ç»‘èµ„æº
  CHECK(UnbindTaskFromResource(td_ptr, res_id_tmp));
  
  // 2. è®¾ç½®ä¸ºå¤±è´¥çŠ¶æ€
  td_ptr->set_state(TaskDescriptor::FAILED);
  
  // 3. â­ å¦‚æœæœªè¶…è¿‡é‡è¯•é™åˆ¶ï¼Œé‡æ–°è°ƒåº¦
  if (!exceeded_retry_limit) {
    scheduler::SchedulerStats scheduler_stats;
    ScheduleJob(jd, &scheduler_stats);  // é‡æ–°è°ƒåº¦
  }
}
```

### 1.5 èŠ‚ç‚¹æ•…éšœå¤„ç†
```cpp
void FlowScheduler::HandleTasksFromDeregisteredResource(
    ResourceTopologyNodeDescriptor* rtnd_ptr) {
  vector<TaskID_t> tasks = BoundTasksForResource(res_id);
  
  for (auto& task_id : tasks) {
    if (FLAGS_reschedule_tasks_upon_node_failure) {
      // â­ é‡æ–°è°ƒåº¦å¤±è´¥èŠ‚ç‚¹ä¸Šçš„ä»»åŠ¡
      HandleTaskEviction(td_ptr, rd_ptr);
    } else {
      HandleTaskFailure(td_ptr);
    }
  }
}
```

---

## 2ï¸âƒ£ Mesos DRF èµ„æºç®¡ç†æœºåˆ¶

### æºç ä½ç½®
- `baselines/mesos/src/master/allocator/mesos/hierarchical.cpp`
- `baselines/mesos/src/master/master.cpp`

### æ ¸å¿ƒå‡½æ•°

#### 2.1 èµ„æºå›æ”¶ - `recoverResources()`
```cpp
void HierarchicalAllocatorProcess::recoverResources(
    const FrameworkID& frameworkId,
    const SlaveID& slaveId,
    const Resources& resources,
    const Option<Filters>& filters,
    bool isAllocated)
{
  // 1. æ£€æŸ¥èµ„æºæ˜¯å¦ä¸ºç©º
  if (resources.empty()) {
    return;
  }
  
  // 2. â­ æ›´æ–° slave çš„å·²åˆ†é…èµ„æºç»Ÿè®¡
  if (isAllocated && slave.isSome()) {
    (*slave)->totalAllocated -= resources;
    roleTree.untrackAllocated(slaveId, resources);
  }
  
  // 3. â­ å¢åŠ  slave çš„å¯ç”¨èµ„æº
  (*slave)->increaseAvailable(frameworkId, resources);
  
  VLOG(1) << "Recovered " << resources 
          << " on agent " << slaveId 
          << " from framework " << frameworkId;
  
  // 4. â­ æ›´æ–° role sorterï¼ˆDRF æ ¸å¿ƒï¼‰
  Sorter* frameworkSorter = CHECK_NOTNONE(getFrameworkSorter(role));
  if (frameworkSorter->contains(frameworkId.value())) {
    untrackAllocatedResources(slaveId, frameworkId, resources);
  }
  
  // 5. å¯é€‰ï¼šå®‰è£…è¿‡æ»¤å™¨ï¼ˆé˜²æ­¢ç«‹å³é‡æ–°åˆ†é…ï¼‰
  if (filters.isSome() && timeout.get() != Duration::zero()) {
    // åˆ›å»º RefusedOfferFilter
    // åœ¨ timeout åè¿‡æœŸ
  }
}
```

#### 2.2 èµ„æºåˆ†é…è·Ÿè¸ª
```cpp
void HierarchicalAllocatorProcess::transitionOfferedToAllocated(
    const SlaveID& slaveId,
    const Resources& resources)
{
  // ä» "offered" è½¬ä¸º "allocated"
  CHECK_NOTNONE(getSlave(slaveId))->totalAllocated += resources;
  roleTree.trackAllocated(slaveId, resources);
}
```

#### 2.3 å–æ¶ˆåˆ†é…è·Ÿè¸ª
```cpp
void HierarchicalAllocatorProcess::untrackAllocatedResources(
    const SlaveID& slaveId,
    const FrameworkID& frameworkId,
    const Resources& allocated)
{
  // ä»å„ä¸ª sorter ä¸­ç§»é™¤åˆ†é…è®°å½•
  foreachpair (const string& role,
               const Resources& allocation,
               allocated.allocations()) {
    Sorter* frameworkSorter = CHECK_NOTNONE(getFrameworkSorter(role));
    
    // â­ ä» DRF sorter ä¸­ç§»é™¤
    frameworkSorter->unallocated(
        frameworkId.value(), slaveId, allocation);
    
    // â­ ä» role sorter ä¸­ç§»é™¤
    roleSorter->unallocated(role, slaveId, allocation);
  }
}
```

### 2.4 Mesos Master çš„ä»»åŠ¡çŠ¶æ€ç®¡ç†

ä»»åŠ¡å®Œæˆæ—¶çš„è°ƒç”¨é“¾ï¼š
```cpp
Master::statusUpdate()
  -> Framework::update()
  -> allocator->recoverResources()  // â­ é‡Šæ”¾èµ„æº
```

ä»»åŠ¡å¤±è´¥é‡è¯•ï¼š
```cpp
Master::_launchTasks()
  -> if (task_launch_failed) {
       // â­ èµ„æºç«‹å³å›æ”¶
       allocator->recoverResources(frameworkId, slaveId, resources);
       
       // â­ é€šçŸ¥ framework ä»»åŠ¡å¤±è´¥
       framework->taskLost(taskId, slaveId, REASON_TASK_INVALID);
     }
```

---

## 3ï¸âƒ£ æˆ‘ä»¬çš„å®ç° vs çœŸå®ç³»ç»Ÿ

### âŒ **é—®é¢˜ 1ï¼šæ— èµ„æºé‡Šæ”¾**

#### æˆ‘ä»¬çš„å®ç°
```python
def run_mesos_drf(tasks, num_machines):
    # ...
    for task_id, machine_id in placements:
        machines[machine_id].cpu_used += task.cpu  # â­ åªå¢åŠ ï¼Œä¸å‡å°‘
        machines[machine_id].mem_used += task.mem
        machines[machine_id].tasks.append((task_id, task.tenant))
    
    # âŒ æ²¡æœ‰èµ„æºé‡Šæ”¾é€»è¾‘
    # âŒ ä»»åŠ¡æ°¸ä¹…å ç”¨èµ„æº
    return {"scheduled": len(placements), ...}
```

#### çœŸå®ç³»ç»Ÿ
```cpp
// ä»»åŠ¡å®Œæˆæ—¶
recoverResources(frameworkId, slaveId, resources)
  -> slave->increaseAvailable(resources);      // â­ å¢åŠ å¯ç”¨èµ„æº
  -> slave->totalAllocated -= resources;       // â­ å‡å°‘å·²åˆ†é…
  -> frameworkSorter->unallocated(...);        // â­ æ›´æ–° DRF çŠ¶æ€
```

### âŒ **é—®é¢˜ 2ï¼šæ— é‡è¯•æœºåˆ¶**

#### æˆ‘ä»¬çš„å®ç°
```python
def run_tetris(tasks, num_machines):
    for task in tasks:
        placed = False
        for machine in machines:
            if can_fit(machine, task):
                place_task(machine, task)
                placed = True
                scheduled += 1
                break
        
        if not placed:
            failed += 1  # âŒ ç›´æ¥æ ‡è®°å¤±è´¥ï¼Œä¸é‡è¯•
    
    return {"failed": failed, ...}
```

#### çœŸå®ç³»ç»Ÿ
```cpp
// Firmament
void HandleTaskEviction(task) {
  UnbindTaskFromResource(task);
  task->set_state(RUNNABLE);
  InsertTaskIntoRunnables(task);  // â­ é‡æ–°åŠ å…¥è°ƒåº¦é˜Ÿåˆ—
}

// Mesos
Master::_statusUpdate() {
  if (status == TASK_FAILED && retries < MAX_RETRIES) {
    // â­ é‡æ–°æäº¤ä»»åŠ¡
    framework->resubmitTask(taskId);
  }
}
```

### âŒ **é—®é¢˜ 3ï¼šé™æ€å¿«ç…§ vs åŠ¨æ€äº‹ä»¶é©±åŠ¨**

#### æˆ‘ä»¬çš„å®ç°
```python
# ä¸€æ¬¡æ€§è°ƒåº¦æ‰€æœ‰ä»»åŠ¡ï¼ˆé™æ€å¿«ç…§ï¼‰
placements = allocator.allocate(all_tasks)

# âŒ æ²¡æœ‰æ—¶é—´ç»´åº¦
# âŒ æ²¡æœ‰ä»»åŠ¡å®Œæˆäº‹ä»¶
# âŒ æ²¡æœ‰èµ„æºå›æ”¶å‘¨æœŸ
```

#### çœŸå®ç³»ç»Ÿ
```cpp
// äº‹ä»¶é©±åŠ¨æ¶æ„
class EventDrivenScheduler {
  // æŒç»­ç›‘å¬äº‹ä»¶
  void HandleTaskCompletion(task) { ... }
  void HandleTaskEviction(task) { ... }
  void HandleTaskFailure(task) { ... }
  void HandleNodeFailure(node) { ... }
  
  // â­ å‘¨æœŸæ€§é‡æ–°è°ƒåº¦
  void ScheduleAllJobs() { 
    while (æœ‰ runnable ä»»åŠ¡) {
      åˆ†é…èµ„æº -> è¿è¡Œä»»åŠ¡ -> ç­‰å¾…å®Œæˆ -> é‡Šæ”¾èµ„æº
    }
  }
}
```

---

## 4ï¸âƒ£ å¯¹æ¯”è¡¨æ ¼

| ç‰¹æ€§ | Firmament (çœŸå®) | Mesos (çœŸå®) | æˆ‘ä»¬çš„æ¨¡æ‹Ÿ âŒ |
|------|-----------------|-------------|------------|
| **èµ„æºé‡Šæ”¾** | âœ… `UnbindTaskFromResource()` | âœ… `recoverResources()` | âŒ æ—  |
| **ä»»åŠ¡å®Œæˆäº‹ä»¶** | âœ… `HandleTaskCompletion()` | âœ… `statusUpdate()` | âŒ æ—  |
| **å¤±è´¥é‡è¯•** | âœ… é‡æ–°åŠ å…¥ `runnable_tasks_` | âœ… `resubmitTask()` | âŒ æ—  |
| **é©±é€é‡è°ƒåº¦** | âœ… `HandleTaskEviction()` | âœ… èµ„æºç«‹å³å›æ”¶ | âŒ æ—  |
| **èŠ‚ç‚¹æ•…éšœå¤„ç†** | âœ… `HandleTasksFromDeregisteredResource()` | âœ… `taskLost()` | âŒ æ—  |
| **æ—¶é—´ç»´åº¦** | âœ… äº‹ä»¶é©±åŠ¨ + å‘¨æœŸè°ƒåº¦ | âœ… å¼‚æ­¥æ¶ˆæ¯é©±åŠ¨ | âŒ é™æ€å¿«ç…§ |
| **DRF çŠ¶æ€æ›´æ–°** | âœ… Flow graph åŠ¨æ€æ›´æ–° | âœ… `Sorter::allocated/unallocated()` | âŒ ä¸€æ¬¡æ€§è®¡ç®— |
| **èµ„æºè¿‡æ»¤å™¨** | âœ… æ”¯æŒï¼ˆé¿å…ç«‹å³é‡åˆ†é…ï¼‰ | âœ… `RefusedOfferFilter` | âŒ æ—  |
| **ä¼˜å…ˆçº§æŠ¢å ** | âœ… `HandleTaskEviction()` | âœ… Offer æ‹’ç»æœºåˆ¶ | âŒ æ—  |

---

## 5ï¸âƒ£ å½±å“åˆ†æ

### å¯¹å®éªŒç»“æœçš„å½±å“

#### 1. **èµ„æºåˆ©ç”¨ç‡ä¸¥é‡ä½ä¼°**
```python
# æˆ‘ä»¬çš„æ¨¡æ‹Ÿ
è°ƒåº¦ 1000 ä»»åŠ¡ -> èµ„æºæ°¸ä¹…å ç”¨ -> åç»­ä»»åŠ¡å¤±è´¥ -> åˆ©ç”¨ç‡ 50%

# çœŸå®ç³»ç»Ÿ
è°ƒåº¦ 1000 ä»»åŠ¡ -> è¿è¡Œ + å®Œæˆ -> èµ„æºé‡Šæ”¾ -> ç»§ç»­è°ƒåº¦ -> åˆ©ç”¨ç‡ 90%+
```

#### 2. **è°ƒåº¦æˆåŠŸç‡ä¸å…¬å¹³**
- **NextGen**: æœ‰åŠ¨æ€èµ„æºé‡Šæ”¾ â†’ æˆåŠŸç‡é«˜ âœ…
- **Firmament/Mesos**: æ— èµ„æºé‡Šæ”¾ â†’ æˆåŠŸç‡ä½ âŒ
- **æ¯”è¾ƒä¸å…¬å¹³**ï¼šNextGen æœ‰é¢å¤–ä¼˜åŠ¿

#### 3. **æ— æ³•æ¨¡æ‹Ÿé•¿æ—¶é—´è¿è¡Œ**
```python
# çœŸå®é›†ç¾¤è¿è¡Œ 7 å¤©
Day 1: è°ƒåº¦ + è¿è¡Œ + é‡Šæ”¾ -> å¾ªç¯åˆ©ç”¨èµ„æº
Day 7: ä»åœ¨æ­£å¸¸è¿è¡Œ

# æˆ‘ä»¬çš„æ¨¡æ‹Ÿ
å‰ 10 åˆ†é’Ÿ: è°ƒåº¦æˆåŠŸ
åé¢æ—¶é—´: èµ„æºè€—å°½ï¼Œæ— æ³•ç»§ç»­
```

---

## 6ï¸âƒ£ ä¿®å¤å»ºè®®

### æ–¹æ¡ˆ Aï¼šä¸ºæ‰€æœ‰ç®—æ³•æ·»åŠ åŠ¨æ€èµ„æºç®¡ç† âœ… **æ¨è**

```python
class UnifiedDynamicScheduler:
    def run_with_dynamic_release(self, algorithm_func, tasks, machines):
        current_time = 0
        active_tasks = []  # (task_id, end_time, machine_id, resources)
        
        while has_pending_tasks or has_active_tasks:
            # 1. â­ é‡Šæ”¾å·²å®Œæˆä»»åŠ¡
            for task in active_tasks:
                if task.end_time <= current_time:
                    release_resources(task)
                    active_tasks.remove(task)
            
            # 2. è¿è¡Œè°ƒåº¦ç®—æ³•
            placements = algorithm_func(pending_tasks, machines)
            
            # 3. è®°å½•ä»»åŠ¡ç»“æŸæ—¶é—´
            for task_id, machine_id in placements:
                end_time = current_time + task.duration
                active_tasks.append((task_id, end_time, machine_id, ...))
            
            # 4. æ¨è¿›æ—¶é—´
            current_time = next_event_time()
```

### æ–¹æ¡ˆ Bï¼šä¸ºæ‰€æœ‰ç®—æ³•æ·»åŠ é‡è¯•æœºåˆ¶

```python
def schedule_with_retry(tasks, machines, max_retries=3):
    pending = deque(tasks)
    retry_queue = []
    
    while pending or retry_queue:
        task = pending.popleft() if pending else retry_queue.pop(0)
        
        if place_task(task, machines):
            scheduled.append(task)
        else:
            if task.retries < max_retries:
                task.retries += 1
                retry_queue.append(task)
            else:
                failed.append(task)
```

### æ–¹æ¡ˆ Cï¼šä½¿ç”¨çœŸå® trace çš„æ—¶é—´æˆ³

```python
# ä» batch_instance.csv è¯»å–
task.start_time  = row[5]   # çœŸå®å¼€å§‹æ—¶é—´
task.end_time    = row[6]   # çœŸå®ç»“æŸæ—¶é—´
task.duration    = end_time - start_time

# æŒ‰æ—¶é—´é¡ºåºæ¨¡æ‹Ÿ
for timestamp in sorted(all_timestamps):
    release_completed_tasks(timestamp)
    schedule_new_arrivals(timestamp)
```

---

## 7ï¸âƒ£ ç»“è®º

### âœ… çœŸå®ç³»ç»Ÿçš„ç‰¹ç‚¹

1. **åŠ¨æ€äº‹ä»¶é©±åŠ¨**ï¼šä»»åŠ¡å®Œæˆ â†’ èµ„æºé‡Šæ”¾ â†’ ç»§ç»­è°ƒåº¦
2. **å®Œå–„çš„é‡è¯•æœºåˆ¶**ï¼šå¤±è´¥/é©±é€ â†’ é‡æ–°åŠ å…¥é˜Ÿåˆ— â†’ å†æ¬¡è°ƒåº¦
3. **æ—¶é—´ç»´åº¦**ï¼šæŒç»­è¿è¡Œï¼Œèµ„æºä¸æ–­å¾ªç¯åˆ©ç”¨
4. **çŠ¶æ€ç»´æŠ¤**ï¼šå®æ—¶æ›´æ–° DRF dominant shareã€Flow graph

### âŒ æˆ‘ä»¬æ¨¡æ‹Ÿçš„é—®é¢˜

1. **é™æ€å¿«ç…§**ï¼šä¸€æ¬¡æ€§åˆ†é…æ‰€æœ‰ä»»åŠ¡
2. **æ— èµ„æºé‡Šæ”¾**ï¼šä»»åŠ¡æ°¸ä¹…å ç”¨èµ„æº
3. **æ— é‡è¯•æœºåˆ¶**ï¼šå¤±è´¥å³å¤±è´¥
4. **ä¸å…¬å¹³æ¯”è¾ƒ**ï¼šåªæœ‰ NextGen æœ‰åŠ¨æ€ç®¡ç†

### ğŸ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

**å»ºè®®ç«‹å³å®æ–½æ–¹æ¡ˆ A + C**ï¼š
- âœ… å·²ä¸º NextGen å®ç°åŠ¨æ€èµ„æºç®¡ç†
- âš ï¸ **éœ€è¦ä¸º Firmamentã€Mesosã€Tetris ä¹Ÿå®ç°ç›¸åŒæœºåˆ¶**
- âš ï¸ ä½¿ç”¨çœŸå®çš„ä»»åŠ¡æ—¶é•¿æ•°æ®
- âš ï¸ ç»Ÿä¸€æ‰€æœ‰ç®—æ³•çš„è¯„ä¼°æ ‡å‡†

è¿™æ ·æ‰èƒ½è¿›è¡Œ**å…¬å¹³ã€æœ‰æ„ä¹‰çš„å¯¹æ¯”**ï¼

---

**å‚è€ƒæºç **:
- Firmament: `baselines/firmament/src/scheduling/event_driven_scheduler.cc`
- Mesos: `baselines/mesos/src/master/allocator/mesos/hierarchical.cpp`
- æˆ‘ä»¬çš„å®ç°: `tools/run_complete_comparison.py`


